{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Recommender system : Prediction using similar users' ratings\n",
    "## Books for mystery, thriller, and crime\n",
    "https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/adshah/Documents/Python/Jupyter Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/Users/adshah/Documents/Python/Jupyter Notebooks/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) General information for all books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_df = pd.read_csv('data/book_id_map.csv')\n",
    "print(book_id_df.shape)\n",
    "book_id_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_df = pd.read_csv('data/user_id_map.csv')\n",
    "print(user_id_df.shape)\n",
    "user_id_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pd.read_csv('data/goodreads_book_authors.csv')\n",
    "print(authors_df.shape)\n",
    "authors_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_interactions_df = pd.read_csv('data/goodreads_interactions.csv')\n",
    "print(all_interactions_df.shape)\n",
    "all_interactions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Specific information for mystery, thriller, and crim books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv('data/goodreads_books_mystery_thriller_crime.csv')\n",
    "books_df.authors = books_df.authors.str.split(pat = \",\").str[0].str.split(pat=\":\").str[1].str.split(pat=\"'\").str[1]\n",
    "books_df = books_df.rename(columns={'authors': 'author_id'})\n",
    "books_df.similar_books = books_df.similar_books.str.replace(\"^\\[|\\]$\",\"\")\n",
    "print(books_df.shape)\n",
    "books_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.read_csv('data/goodreads_interactions_mystery_thriller_crime.csv')\n",
    "print(interactions_df.shape)\n",
    "interactions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('data/goodreads_reviews_mystery_thriller_crime.csv')\n",
    "print(reviews_df.shape)\n",
    "reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map users to book data\n",
    "books_users=interactions_df[interactions_df['is_read']==True].merge(books_df,how='left',on='book_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now use the same data cleaning steps as we performed in the \"Test Train Split\" notebook. Note that the training data that we create there pnly includes user_id|book_id|rating columns which are appropriate for collaborative filtering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df=books_users.groupby('user_id').book_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df=grouped_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_irrelevant=grouped_df[grouped_df['book_id']<3]['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_users=books_users[~books_users['user_id'].isin(users_irrelevant)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP : map all users to a n-dimensional space based on their book reading habits. This can be captured by using text processing techniques on book titles and descriptions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column book info that concatenates book title and description. This is the primary column we will be using\n",
    "books_users['book_info']=books_users['title'].map(str)+' '+books_users['description'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major problem in splitting text into words is not being able to say that words like \"reading\" and \"read\" are same. To improve this, we tried both lemmatization and stemming techniques. However, finally decided to do Snowball stemming before TF-IDF vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "eng_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfIdfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfIdfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([eng_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to try count vectorizer\n",
    "# vectorizer =  StemmedCountVectorizer(min_df=0.1,max_df=0.8,analyzer=\"word\", stop_words='english') #tune\n",
    "# X = vectorizer.fit_transform(books_users['book_info'])\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =  StemmedTfIdfVectorizer(min_df=0.1,max_df=0.8,analyzer=\"word\", stop_words='english') #tune\n",
    "X = vectorizer.fit_transform(books_users['book_info'])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=X.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index=books_users['user_id'].map(str)+'-'+books_users['book_id'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "#create pickle file for faster data retrieval\n",
    "df.to_pickle('data/tfidf_vect_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have tf-idf vectors for every user-book combination. However, we want to combine them to get one vector per user for K Means. In the following code, we create the mean tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_pickle('data/tfidf_vect_df.pkl')\n",
    "tfidf_df=tfidf_df.reset_index()\n",
    "tfidf_df[['user_id','book-id']]=tfidf_df['index'].str.split('-',expand=True,n=1)\n",
    "tfidf_df_temp=tfidf_df.copy()\n",
    "del tfidf_df_temp['book-id']\n",
    "tfidf_avg_df=tfidf_df_temp.groupby('user_id').mean()\n",
    "tfidf_avg_df.to_pickle('data/final_tfidf_avg_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_avg_df = pd.read_pickle('data/final_tfidf_avg_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for clustering. We use 2 clustering methods - KMeans and DBSCAN. \n",
    "DBSCAN is a better algorithm to run here (no initialization for the number of clusters required). However due to the size of the dataset, it was computationally heavy.\n",
    "To find the best \"k\" for KMeans, we use \"silhouette scores\" for K=2 to 500 and chose the optimal K accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tfidf_avg_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "The average silhouette_score is :\n",
      "0.09503862496960176\n",
      "5\n",
      "The average silhouette_score is :\n",
      "0.10322853262673345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "range_clusters=[2,5]\n",
    "for n in range_clusters:\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n,random_state=0,batch_size=6,max_iter=10,init='k-means++')\n",
    "    cluster_labels=kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#     print(\"For n_clusters =\"+ str(n) + \"The average silhouette_score is :\" + silhouette_avg)\n",
    "    print(n)\n",
    "    print(\"The average silhouette_score is :\")\n",
    "    print(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "range_clusters=[20,35,50,100,200,350,500]\n",
    "for n in range_clusters:\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n,random_state=0,batch_size=6,max_iter=10,init='k-means++')\n",
    "    cluster_labels=kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#     print(\"For n_clusters =\"+ str(n) + \"The average silhouette_score is :\" + silhouette_avg)\n",
    "    print(n)\n",
    "    print(\"The average silhouette_score is :\")\n",
    "    print(silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above hyperparameter tuning gave an optimal K of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=5,random_state=0,batch_size=6,max_iter=10,init='k-means++')\n",
    "cluster_labels=kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to generate final cluster assignments to users\n",
    "user_cluster=pd.DataFrame(columns=['user_id','cluster'])\n",
    "user_cluster['user_id']=tfidf_avg_df.index\n",
    "user_cluster['cluster']=cluster_labels\n",
    "user_cluster.set_index('user_id',inplace=True)\n",
    "user_cluster.to_csv('user_cluster_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried dimensionality reduction to increase the speed of DBSCAN run. However, it led to memory errors on HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=15)\n",
    "X_pca=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "start=time.time()\n",
    "db=DBSCAN(eps=0.8,min_samples=4).fit(X_pca)\n",
    "end=time.time()\n",
    "print(\"Total time required to go over full data: \",end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
